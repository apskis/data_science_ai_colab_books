{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apskis/data_science_ai_colab_books/blob/main/HuggingFace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLMs for Coding Assistance\n",
        "\n",
        "## Google Gemini\n",
        "- Model family: Gemini 2.5 / Gemini 3 Pro\n",
        "\n",
        "- Coding focus: Gemini Code Assist integrates into IDEs (VS Code, JetBrains) to provide:\n",
        "\n",
        "  - Code generation and completion\n",
        "\n",
        "  - Conversational help (explain snippets, suggest edits)\n",
        "\n",
        "## GitHub Copilot\n",
        "\n",
        "- Powered by: OpenAI Codex / GPT‑4‑class models\n",
        "\n",
        "- Coding focus: Acts as an AI pair programmer inside your IDE.\n",
        "\n",
        "\n",
        "# Introduction to Hugging Face\n",
        "\n",
        "## What Hugging Face Is\n",
        "- Community + Platform: Hugging Face is both a company and an open‑source community focused on democratizing AI.\n",
        "\n",
        "- Model Hub: A central repository where thousands of pretrained models are hosted — from text classification to image generation.\n",
        "\n",
        "- Open Source Libraries: Hugging Face maintains widely used libraries like:\n",
        "\n",
        "  - Transformers: access to state‑of‑the‑art NLP and LLMs.\n",
        "  - Datasets: curated datasets for training and evaluation.\n",
        "  - Tokenizers: efficient text preprocessing.\n",
        "  - Accelerate: tools for distributed training.\n",
        "\n",
        "## Why Hugging Face Matters\n",
        "- Accessibility: Makes cutting‑edge AI models available to everyone, not just big tech labs.\n",
        "\n",
        "- Collaboration: Researchers and developers can upload, share, and fine‑tune models.\n",
        "\n",
        "- Standardization: Provides consistent APIs across models, making experimentation easier.\n",
        "\n",
        "- Education: Offers free courses and tutorials to help beginners learn about LLMs and NLP.\n",
        "\n",
        "## What You Can Do with Hugging Face\n",
        "- Load Pretrained Models: Use models like BERT, GPT‑2, or Stable Diffusion with just a few lines of code.\n",
        "\n",
        "- Fine‑Tune Models: Adapt existing models to your own dataset.\n",
        "\n",
        "- Deploy Models: Host models on Hugging Face Spaces (interactive apps built with Gradio/Streamlit).\n",
        "\n",
        "- Benchmark & Compare: Evaluate models across tasks using shared datasets and leaderboards.\n",
        "\n"
      ],
      "metadata": {
        "id": "VLucR4HApDGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "y2DGYiznAHpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline('sentiment-analysis')\n",
        "\n",
        "res = classifier(\"I love Sundays\")"
      ],
      "metadata": {
        "id": "ZpX8QOtqAHI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = classifier(\"I love waiting a long time for my food at restaurants?\")\n",
        "res"
      ],
      "metadata": {
        "id": "sR7wo7utAFjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_5 = '''\n",
        "Edit 11/9/2025: Upping this to 5 stars. PlayStation released cloud streaming recently and it is AMAZING. All the connectivity issues with Remote Play back to my PS5 are nonexistent with Cloud Streaming, and there are nearly 3,000 games available. The Portal finally feels like a standalone handheld console, rather than being tethered to a (sometimes) finicky Remote Play connection\n",
        "\n",
        "I really wanted to use this device more. I loved the idea of having the ability to play my PlayStation 5 from the couch with something else on TV. However, there are some limitations:\n",
        "\n",
        "1) Your PS5 must be plugged in via ethernet for this to work well. For whatever reason, the WiFi on this device isn't as strong as something like an iPhone when my PlayStation 5 was connected via WiFi.\n",
        "\n",
        "2) Not a great device for traveling. Due to the device size, it's hard (especially adding a case) to fit this in a smaller bag or easily pull out on a plane. Obviously, this is limited too by the fact that it needs to connect back to your PS5, and I had pretty consistent issues trying to use hotel WiFi or hotspots to connect.\n",
        "\n",
        "If you don't mind the above flaws, this device feels great, the feedback is great, and as a controller it's the closest thing to a DualSense controller. My only gripe with the controller itself is the sticks are smaller than they are on normal controller, and I would've liked the bigger size.\n",
        "'''\n",
        "\n",
        "res = classifier(review_5)\n",
        "res"
      ],
      "metadata": {
        "id": "tryRQMOFAGaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline('sentiment-analysis',\n",
        "                      model='cardiffnlp/twitter-roberta-base-sentiment')\n",
        "\n",
        "res = classifier(review_5)\n",
        "res"
      ],
      "metadata": {
        "id": "8qknpRLeu3uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet = 'I love the new iPhone!'\n",
        "res = classifier(tweet)\n",
        "res"
      ],
      "metadata": {
        "id": "Z35j8_Ouu3aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet = '@Airline, my flight was canceled AGAIN, terrible service!\" (negative)'\n",
        "res = classifier(tweet)\n",
        "res"
      ],
      "metadata": {
        "id": "gwjUV388u3Ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet = 'The new Google logo is blah'\n",
        "res = classifier(tweet)\n",
        "res"
      ],
      "metadata": {
        "id": "il1BgDj3v5el"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text generation\n",
        "\n",
        "generator = pipeline('text-generation', model='distilgpt2')"
      ],
      "metadata": {
        "id": "zHjdyaExxpis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Data science empowers businesses because'\n",
        "\n",
        "res = generator(text, max_length=40, num_return_sequences=1)\n",
        "res"
      ],
      "metadata": {
        "id": "Qao8IZTdxpJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline('text-generation', model='gpt2')"
      ],
      "metadata": {
        "id": "B_ivFIgoyH8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c\n",
        "res"
      ],
      "metadata": {
        "id": "R3wBYYJju2kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(res[0]['generated_text'])"
      ],
      "metadata": {
        "id": "xedvJ6fQy9oN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline('text-generation', model='openai-community/gpt2-medium')"
      ],
      "metadata": {
        "id": "bvjNapT-zRqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = generator(text, max_length=40, num_return_sequences=1)\n",
        "print(res[0]['generated_text'])"
      ],
      "metadata": {
        "id": "gjt4pXzNzRUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline('text-generation', model='openai-community/gpt2-large')"
      ],
      "metadata": {
        "id": "pnabLqt-0OT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = generator(text, max_length=40, num_return_sequences=1)\n",
        "print(res[0]['generated_text'])"
      ],
      "metadata": {
        "id": "vlJagTh50svE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# distillgpt2 - 88.2 million parameters\n",
        "# gpt 2 - 124 million parameters\n",
        "# gpt 2 - medium - 345 million parameters\n",
        "# gpt 2 - large - 774 million parameters"
      ],
      "metadata": {
        "id": "SK_LzQNFyx52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# named entity recognition\n",
        "\n",
        "ner = pipeline('ner', grouped_entities=True)"
      ],
      "metadata": {
        "id": "38CnFgfF2MHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Jeff Herman played for the Chicago Bulls and is from North Carolina\"\n",
        "res = ner(text)\n",
        "res"
      ],
      "metadata": {
        "id": "OeV7G4qh2e62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_pipeline = pipeline('question-answering',\n",
        "                       model='distilbert/distilbert-base-cased-distilled-squad')"
      ],
      "metadata": {
        "id": "2dqEF6du34hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = 'Is Paris the largest city in France'\n",
        "\n",
        "context = '''Paris is the capital of France and the largest city in France.\n",
        " It is located on the river Seine.'''\n",
        "\n",
        "result = qa_pipeline(question=question, context=context)\n",
        "result"
      ],
      "metadata": {
        "id": "C_etpJjO3kBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_pipeline = pipeline('question-answering',\n",
        "                       model='deepset/roberta-base-squad2')"
      ],
      "metadata": {
        "id": "od3iVLlI6RIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = 'What is the capital of France?'\n",
        "\n",
        "context = '''Paris is the capital of France and the largest city in France.\n",
        " It is located on the river Seine.'''\n",
        "\n",
        "result = qa_pipeline(question=question, context=context)\n",
        "result"
      ],
      "metadata": {
        "id": "O2rO1-dR6XFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chat bot\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# this model is trained on reddit conversations\n",
        "# Load DialoGPT-medium\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")\n",
        "\n",
        "# Start a conversation\n",
        "user_input = \"What's the capital of France?\"\n",
        "new_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "# Generate a reply\n",
        "chat_history_ids = model.generate(\n",
        "    new_input_ids,\n",
        "    max_length=1000,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Decode only the new response (skip the input)\n",
        "reply = tokenizer.decode(chat_history_ids[:, new_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "print(\"Bot:\", reply)"
      ],
      "metadata": {
        "id": "T3S6t2lt8WAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize chat history\n",
        "chat_history_ids = None\n",
        "\n",
        "print(\"Chatbot ready! Type 'quit' to stop.\\n\")\n",
        "\n",
        "while True:\n",
        "    # Get user input\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"quit\", \"exit\"]:\n",
        "        print(\"Chatbot session ended.\")\n",
        "        break\n",
        "\n",
        "    # Encode new user input, append to chat history\n",
        "    new_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "    # If there’s previous history, append it\n",
        "    bot_input_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1) if chat_history_ids is not None else new_input_ids\n",
        "\n",
        "    # Generate response\n",
        "    chat_history_ids = model.generate(\n",
        "        bot_input_ids,\n",
        "        max_length=1000,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Decode only the new response\n",
        "    reply = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "    print(\"Bot:\", reply)"
      ],
      "metadata": {
        "id": "2YW1bcgc-Pkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resources\n",
        "- [Finetuning LLM models - CodeAcademy](https://www.codecademy.com/learn/finetuning-transformer-models)\n",
        "- [Hugging Face LLM Course](https://huggingface.co/learn/llm-course/chapter1/1)\n",
        "- [Hugging Face Course Library](https://huggingface.co/learn)"
      ],
      "metadata": {
        "id": "KzyzC-cE8L8p"
      }
    }
  ]
}